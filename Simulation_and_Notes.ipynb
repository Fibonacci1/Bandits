{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "caab4d946885da152b6a2ee6aa03d96dca06c79349a91b8f2aea1f8bf8667d8c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bandit Algorithms\n",
    "\n",
    "Online decision-making involves a chioce of **exploration-expliotation** and bandits model have widely applications likes: ...\n",
    "\n",
    "The goal is: maximize expected cumulative reward.\n",
    "\n",
    "The reward model:\n",
    "\n",
    "1. **IID reward:**\n",
    "2. **Adversarial reward:**\n",
    "3. **Constrained adversarial reward:**\n",
    "4. **Stochastic reward(beyond IID):** random process, e.g., a random walk.\n",
    "\n",
    "## Stochastic bandits\n",
    "\n",
    "In each round, the agent can only observe the reward of chosen action and the rewards are bounded. Every time an action $a$ is chosen, the reward is sampled independently from the reward distribution associated with the action $a$.\n",
    "A multi-armed bandit is the tuple $<\\mathcal{A}, \\mathcal{R}>$ where $\\mathcal{A}$ is the set of actions(arms) and the $\\mathcal{R}$ is the unknown reward distribution, $\\mathcal{R}^a(r)= \\mathbb{P}[r|a]$. At each round $t$, an agent select the action $a_t \\in \\mathcal{R}$, receive a reward $r_t \\sim \\mathcal{R}^a(t)$\n",
    "\n",
    "## UCB algorithm\n",
    "\n",
    "## Thompson sampling\n",
    "\n",
    "## Gradient bandit\n",
    "\n",
    "## Gaussian process regression\n",
    "\n",
    "## Gaussian processes bandit\n",
    "\n",
    "### GP-UCB\n",
    "\n",
    "### GP-TS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Greedy algorithm\n",
    "\n",
    "We consoder the algorithms that estimate the $Q(a)$ by Monte-Carlo evaluation:\n",
    "$$\\hat{Q}_t(a) = \\frac{1}{N_{t-1}(a)}\\sum\\limits_{\\tau=1}^{t-1}r_{\\tau}\\mathbf{1}_\\tau $$\n",
    "The greedy algorithm selects action with highest estimated value: $$a_t = \\arg\\max_{a\\in \\mathcal{A}} \\hat{Q}_t(a)$$\n",
    "\n",
    "But the greedy algorithm will lock onto the suboptimal action forever, which means the greedy algorithm is given to eploiting rather heavily. So, we need to force it to try other actions by utilizing a random selection with a probability $ \\epsilon $ at each round. At each round $t$, the agent will:\n",
    "* with probability $1-\\epsilon$ select $a_t = \\arg\\max_{a\\in \\mathcal{A}} \\hat{Q}_t(a)$\n",
    "* with probability $\\epsilon$ select a random action.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'cumulate' from 'utils' (/Users/songjiexie/Desktop/Codes/Python/Simple-Bandits/utils.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aa777390edd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbanidts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMutiBernArmEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_algorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepGreedyAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcumulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_arms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cumulate' from 'utils' (/Users/songjiexie/Desktop/Codes/Python/Simple-Bandits/utils.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from banidts.envs import MutiBernArmEnv\n",
    "from algorithms.simple_algorithms import epGreedyAgent\n",
    "from utils import cumulate\n",
    "\n",
    "num_arms = 10\n",
    "T =1000\n",
    "ep_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "actions = list(range(num_arms))\n",
    "ps = np.random.uniform(0,1,size=num_arms)\n",
    "\n",
    "mBern_env_lists= []\n",
    "for i in range(6):\n",
    "    mBern_env_lists.append(\n",
    "        MutiBernArmEnv(actions, ps)\n",
    "    )\n",
    "print(\"Arms: \", mBern_env_lists[1].arms_names)\n",
    "print(\"Optimal arm: \", mBern_env_lists[1].optimal)\n",
    "\n",
    "agents_list = []\n",
    "for ep in ep_list:\n",
    "    agents_list.append(\n",
    "        epGreedyAgent(actions, ep)\n",
    "    )\n",
    "\n",
    "for t in range(T):\n",
    "    for i in range(len(ep_list)):\n",
    "        action = agents_list[i].decide()\n",
    "        reward = mBern_env_lists[i].take(action)\n",
    "        agents_list[i].update(reward)\n",
    "\n",
    "cumulative_regrets_list = []\n",
    "for env in mBern_env_lists:\n",
    "    cumulative_regrets_list.append(\n",
    "        cumulate(env.regrets)\n",
    "    )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cumulative_regrets_list[0], label=\"\\epsilon = 0.1\")\n",
    "plt.plot(cumulative_regrets_list[1], label=\"\\epsilon = 0.2\")\n",
    "plt.plot(cumulative_regrets_list[2], label=\"\\epsilon = 0.3\")\n",
    "plt.plot(cumulative_regrets_list[3], label=\"\\epsilon = 0.4\")\n",
    "plt.plot(cumulative_regrets_list[4], label=\"\\epsilon = 0.5\")\n",
    "plt.plot(cumulative_regrets_list[5], label=\"\\epsilon = 0.6\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}